Q:Is there a way to display what devices are mapped to what interrupt levels (to make it easier to determine who is generating "Interrupt Level n not serviced")? A utility maybe? Or a ddi call? 
A:There seems to be no easy way to list IPL & its ISRs. There is a Sun
unofficial internal utility which prints all the device's IPL. Here is an
example:

# /usr/bin/intrprint


ftp://sunsolve.sun.com:/cores 

mdb command -      ::walk thread | ::findstack

On 09/18/06 20:01, Chad Mynhier wrote:
> I'm trying to debug a Solaris 8 kernel hang with mdb. (This appears
> to be a kernel hang and not simply a process hang, as we can get no
> response from the server and need to drop it to the ok prompt and
> reboot.)
> 
> I've done "::walk thread | ::findstack". Of 95 threads, I get a stack
> trace for 79, and of those, 63 are sitting in cv_wait(). Here are
> some of the stack traces:
> 
> 28
> cv_wait+0x38()
> taskq_thread+0x110()
> thread_start+4()

Ignore - task queue threads waiting for work to be dispatched to them.

> 6
> cv_wait+0x38()
> md_daemon+0x144()
> thread_start+4()

Ignore - metadisk/svm/lvm monitoring threads

> 4
> cv_wait+0x38()
> taskq_thread+0xd4()
> thread_start+4()

Same as above (just a different cv_wait in taskq_thread)

> 4
> cv_wait+0x38()
> sqthread+0x14c()
> thread_start+4()

sqthread is not one I recognise. It does not show up in ON source,
either. I'd guess that having 4 threads cv_wait'ing in here its
likely not a problem.

> 
> 4
> cv_wait+0x38()
> ufs_thread_run+0x130()
> ufs_thread_delete+0x4c()
> thread_start+4()

Ignore.

> My first thought is that the above might be perfectly normal behavior.
> Is there anything I should be suspicious about? If so, how can I
> probe deeper (given that this is Solaris 8)? If not, does anyone have
> any pointers on what I could be looking at?
I'd start by looking at what threads are on cpu (::cpuinfo) and which have
been on cpu recently (t_disp_time relative to panic_lbolt and panic_lbolt64).
Also look for any kernel stacks waiting for locks - ie ending around
mutex_vector_enter; similarly for threads ending in sema_p/sema_wait and in
rw_rdlock, rw_wrlock etc.

> Hi
> 
> On 09/18/06 20:01, Chad Mynhier wrote:
> > I'm trying to debug a Solaris 8 kernel hang with mdb. (This appears
> > to be a kernel hang and not simply a process hang, as we can get no
> > response from the server and need to drop it to the ok prompt and
> > reboot.)
> > 
> > cv_wait+0x38()
> > sqthread+0x14c()
> > thread_start+4()
> 
> sqthread is not one I recognise. It does not show up in ON source,
> either. I'd guess that having 4 threads cv_wait'ing in here its
> likely not a problem.

sqthreads existed in S8 only for background scheduling of squeues. They were 
replaced by task queues in S9. The thread above is just waiting for job to 
arrive, so ignore these as well.

- Alexander Kolbasov


Hmm, ::cpuinfo doesn't seem to be useful in this case, as it looks
like the CPU is only handling the 'sync':

> ::cpuinfo
ID ADDR FLG NRUN BSPL PRI RNRN KRNRN SWITCH THREAD PROC
0 1041b778 1d 0 0 -1 no no t-5 000002a10001fd20 (idle)
2 01991530 1b 0 0 169 no no t-30 000002a100177d20 sched
> 000002a100177d20::findstack
stack pointer for thread 2a100177d20: 2a100176f01
000002a100176fb1 prom_enter_mon+0x38()
000002a100177081 debug_enter+0x15c()
000002a100177151 abort_seq_softintr+0x7c()
000002a100177311 intr_thread+0xa4()
000002a1000131b1 disp_getwork+0xa4()
000002a100013261 idle+0xac()
000002a100013311 thread_start+4()
>

As for which threads have been on CPU recently: Is there some way to
look at t_disp_time on a Solaris 8 server other than dumping memory
starting at the kthread_t pointer and knowing the offset within the
structure?

As for kernel stacks waiting for locks: Here are the functions the
threads are sitting in:

1 disp_getwork+0xa4()
1 intr_thread+0xa4()
2 cv_wait_sig+0x180()
3 cv_wait_sig_swap+0x194()
1 do_scrub_ecache_line+0x8c()
1 sema_p+0x138()
1 current_thread+0x44()
1 cpu_pause+0x7c()
2 cv_timedwait+0x98()
1 putnext+0x1cc()
1 prom_rtt()
1 prom_enter_mon+0x38()
63 cv_wait+0x38()

So there's one thread sitting in sema_p(), but it's the
seg_pasync_thread() thread:

> 2a100007d20::findstack
stack pointer for thread 2a100007d20: 2a100007181
[ 000002a100007181 sema_p+0x138() ]
000002a100007231 seg_pasync_thread+0x104()
000002a100007311 thread_start+4()
>

And from looking at seg_pasync_thread(), I'd guess that this is a normal state.


Hmm, it looks like this thread is in the midst of handling network traffic:

stack pointer for thread 2a10017dd20: 2a10017c941
000002a10017c9f1 putnext+0x1cc()
000002a10017caa1 tcp_wput_slow+0xec0()
000002a10017cbd1 0()
000002a10017cdc1 putnext+0x1cc()
000002a10017ce71 ip_rput_local+0x998()
000002a10017cf61 ip_rput+0x12c4()
000002a10017d031 putnext+0x1cc()
000002a10017d0e1 hmeread+0x33c()
000002a10017d1b1 hmeintr+0x374()
000002a10017d261 pci_intr_wrapper+0x80()
000002a10017d311 intr_thread+0xa4()
000002a100177101 callout_execute+0x84()

WRT the console hang, I'd guess that the zombie state of the console
ttymon might be part of the explanation:

> ::ps -f ! grep ttymon
R 1152 1 1143 1143 0 0x00014208 0000030003f54040
/usr/lib/saf/ttymon
Z 1144 1 1144 1144 0 0x80004208 0000030004cf1540
/usr/lib/saf/ttymon -g -h -p server console login: -T sun -d /de
>

And the zombie state of ttymon is likely a direct result of the zombie
state of init:

> ::ps -f ! grep init
Z 1 0 0 0 0 0x80004208 0000030001b81528 /etc/init -
>

I've done some bug searches, but the ones I see that look promising
were either not reproducible (4040769, 4378228, 4252803) or are in
progress with no useful text I can see (6418709).


>
> > Is there some way to
> > look at t_disp_time on a Solaris 8 server other than dumping memory
> > starting at the kthread_t pointer and knowing the offset within the
> > structure?
>
> Does ::print kthread_t t_disp_time not work on Solaris 8? My memory
> of that release is fading.

Unfortunately, it doesn't:

> 2a10017dd20::print kthread_t t_disp_time
mdb: invalid command '::print': unknown dcmd name
>



What made you think that there is a kernel hang?
- Are the shells/interactive cmds stopped responding
- Is the system not pingable
- not reachable via telnet/rlogin
- did the GUI just freeze
- mouse just doesn't work - cursor does not move
- a few cmds are running eternally
- None of the process related cmds are finishing
- System is not servicing the requests it is supposed to, 
mail/web/file server
- Nothing other than stop-A worked !!

Only if there actually is a hang, will you threads with interesting 
stacks as Gavin
said; From the stacks of the threads of your crash file, there doesn't 
seem to
be a hang !!
-surya


=========Combined command===============
::typegraph
::findlocks   
========================================
> ::modinfo
kmem_flags/X
> ::status
> ::panicinfo
::threadlist 
::threadlist ! grep 300139c6560






A quick cursory glance shows the the following:

> ::walk thread | ::findstack ! munges
88      ##################################  tp: 300072ebb40
        cv_waituntil_sig+0x8c()
        semop+0x548()
        syscall_trap+0x88()

72      ##################################  tp: 2a100017ce0
        taskq_thread+0x110()
        thread_start+4()

54      ##################################  tp: 3000ee13ca0
        rw_enter_sleep+0x144()
        as_pageunlock+0x48()
        default_physio+0x340()
        pread+0x244()
        syscall_trap+0x88()

We have 54 threads waiting for an address space lock.  Not necessarily a 
smoking gun, but a good place to start.  All of these threads are hung up on
the following rwlock:

> 0x3000eda5358::print rwlock_impl_t
{
    rw_wwwh = 0xb
}

So someone has a reader lock and we're all stuck waiting.  Maybe we have a 
recursive rw_enter(RW_READER) somewhere?  This rwlock belongs to the following
address space:

> 3000eda5328::print -a struct as a_lock
{
    3000eda5358 a_lock._opaque = [ 0xb ]
}
> 3000eda5328::as2proc | ::ps
S    PID   PPID   PGID    SID    UID      FLAGS             ADDR NAME
R   2673      1   2673   2673     30 0x00004000 0000030011f0e030 
oracle_mt_x04_fb

So who owns the current reader lock?  With a little ::kgrep | ::whattype magic
the owner appears to be:

> 30013e05220::findstack   
stack pointer for thread 30013e05220: 2a1011a09a1
[ 000002a1011a09a1 sema_p+0x140() ]
  000002a1011a0a51 biowait+0x60()
  000002a1011a0b01 ufs_getpage_miss+0x2f0()
  000002a1011a0c01 ufs_getpage+0x6a8()
  000002a1011a0d61 fop_getpage+0x48()
  000002a1011a0e31 segvn_fault+0x834()
  000002a1011a0fe1 as_fault+0x4a0()
  000002a1011a10f1 pagefault+0xac()
  000002a1011a11b1 trap+0xc14()
  000002a1011a12f1 utl0+0x4c()
>

In this thread, we've taken the address space lock in order to fault in a page,
but UFS is stuck in biowait().  Poking around, we see there are a bunch of
threads similarly stuck:

> ::walk thread | ::findstack ! grep biowait | wc -l
      28

So now something's amiss in the I/O system, which has had a cascading effect on
the VM system, and all hell breaks loose.  The particular buf that we're
interested in is:

> 30006a5a048::print struct buf b_vp | ::vnode2path
/dev/dsk/../../devices/ssm@0,0/pci@18,700000/pci@1/SUNW,isptwo@4/sd@1,0:b

So we're doing some I/O to one of the disks (that's a shocker). There are several 
more suspicious stacks in the dump with traces like:

> 2a1009d9ce0::findstack
stack pointer for thread 2a1009d9ce0: 2a1009d85c1
  000002a1009d8671 swtch+0x78()
  000002a1009d8721 turnstile_block+0x5f8()
  000002a1009d87d1 mutex_vector_enter+0x3dc()
  000002a1009d8881 aio_done+0x1c4()
  000002a1009d8951 volkiodone+0x364()
  000002a1009d8ad1 volsiodone+0x544()
  000002a1009d8bb1 vol_subdisksio_done+0x68()
  000002a1009d8c71 volkcontext_process+0x29c()
  000002a1009d8d41 voldiskiodone+0x368()
  000002a1009d8e41 ssd_return_command+0x198()
  000002a1009d8ef1 ssdintr+0x224()
  000002a1009d8fa1 qlc_fast_fcp_post+0x188()
  000002a1009d9051 qlc_isr+0x378()
  000002a1009d9151 qlc_intr+0xd0()
  000002a1009d9221 pci_intr_wrapper+0x9c()
  000002a1009d92d1 intr_thread+0x130()
  000002a100969171 disp_getwork+0xe8()

This thread is blocked on the following mutex:

> 000002a1009d8881+7ff::print struct frame fr_arg[4]
fr_arg[4] = 0x3001967c060

Which is the aio_mutex for this aio structure:

> 0x3001967c000::print -a aio_t aio_mutex
{
    3001967c060 aio_mutex._opaque = [ 0x30018878c21 ]
}

The owner of this mutex is here:

> 0x30018878c20::findstack
stack pointer for thread 30018878c20: 2a10175cd51
[ 000002a10175cd51 turnstile_block+0x5f8() ]
  000002a10175ce01 rw_enter_sleep+0x144()
  000002a10175ceb1 as_pageunlock+0x48()
  000002a10175cf71 aphysio_unlock+0x6c()
  000002a10175d021 aio_cleanup_cleanupq+0x2c()
  000002a10175d0d1 aio_cleanup+0xc4()
  000002a10175d181 aio_cleanup_thread+0x9c()
  000002a10175d231 kaioc+0xe0()
  000002a10175d2f1 syscall_trap+0x88()
>

Now it seems like we've finally found the culprit.  aio_cleanup_cleanupq() takes
the aio_mutex lock like so:

        [... source code removed ...]

We then call aphysio_unlock().  This particular aio has AIO_PAGELOCKDONE set, so
we go to unlock the page:

        [... source code removed ...]

For as_pageunlock(), we need the address space lock.

The address space lock is held by thread 30013e05220, which is stuck waiting for
I/O to complete.  Now we're in an ugly deadlock, since the I/O can't complete
while the as lock is held.